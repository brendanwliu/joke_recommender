{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joke Recommender\n",
    "## PSTAT 134 Final Project\n",
    "Authors: Brendan Liu (9441056), Elaine Sieng (4279873), Josh Kim (9660796), Yerem Istanboulian (8804528)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Recommender systems are techniques that attempt to suggest relevant items to users from very large and complex datasets. The dataset used includes over 4.1 million continuous ratings (-10.00 to +10.00) of 100 jokes from 73,421 users collected by Ken Goldberg and Theresa Roeder and Dhruv Gupta and Chris Perkins at UC Berkeley. Specifically, our joke recommender system will implement collaborative filtering methods to filter items that a user may like based on similar users. Using model based approaches uncover user-joke interactions which are then used to make further recommendations. This report will discuss specific matrix decomposition methods, component analysis and use a K-nearest neighbors approach to compare to a classic matrix approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Recommender systems are quintessential in the connected world. Many different businesses often face the arduous task of recommending items to users without knowing the preferences of said users. Starting in the 1990’s research in recommender systems have grown significantly and have developed great efficiency in predicting the preferences of users. According to Zhu et al., a recommender system is based on three important pieces of data: users, items (e.g., movies, jokes, news), and user-item usage history (e.g., ratings). The goal is to attempt to understand the relationship between the users, items and user-item history, and to use that understanding to predict future user-item interactions. To tackle these tasks, over the last three decades, researchers have developed methods like non-negative matrix factorization (NMF), K-nearest neighbors (KNN), neural autoencoders, and many more. These models can be further grouped into several categories, among which a very important branch is collaborative filtering (CF).\n",
    "\n",
    "CF can be implemented using many different approaches. The central data source of a CF method is the user-review matrix, with each entry corresponding to a numerical rating of an item within a finite itemset. These matrices tend to be extremely sparse, with many users and items. Our application of matrix methods attempts to find and extract a low ranking representation of the original user-ratings matrix.\n",
    " \n",
    "In our work, we explore three different matrix decomposition methods for topic discovery and user clustering; and, we also utilize a KNN approach to recommending jokes to users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "[Source Code Notebook](Data_Prep.ipynb)\n",
    "\n",
    "[Link to dataset](https://goldberg.berkeley.edu/jester-data/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 100 jokes in the rows and 73,422 users in the columns, each rating is on a continuous scale between -10.0 and 10.0. We were also provided with the actual text of the jokes themselves. The data is curated by the UC Berkeley Laboratory for Automation Science and Engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "**KMeans Clustering of Text - Topic Discovery**\n",
    "\n",
    "**[Source Code Notebook](kmeans-topic-discovery.ipynb)**\n",
    "\n",
    "Our first task was to explore the text data and see if we could discover any topics within our 100 jokes. To make the joke text machine readable, we used the TF-IDF vectorization technique. After vectorizing and normalizing the term-matrix, we used NMF to decompose this matrix instead of tSVD. This is because there is no interpretability of the negative values in tSVD. NMF ensures that the resulting decomposed term frequency matrix has interpretable positive values. \n",
    "NMF decomposes the matrix according to this algorithm:\n",
    "\n",
    "$$ \\min_{W,H} \\| X - WH \\|_F\\\\\n",
    "\\text{ subject to } W\\geq 0,\\ H\\geq 0, $$  \n",
    "    \n",
    "where $W$ is ${p\\times r}$ matrix and $H$ is ${r\\times n}$ matrix. This also ensures only positive values in the resulting matrices.\n",
    "    \n",
    "Our implementation uses r=25 components to decompose the matrix. After implementing NMF, we again passed the decomposed W matrix into the K-means clustering algorithm. We plot the score versus the number of clusters to choose the optimal number of clusters required. The elbow method gives us an idea on what a good k number of clusters would be based on the sum of squared distance (SSE) between data points and their assigned clusters’ centroids. We select k where the SSE starts to flatten out and forms the ‘elbow’. After implementing NMF, we again passed the decomposed W column into the K-means clustering algorithm and used the elbow method to choose the number of clusters. \n",
    "<img src = \"./images/elbow_method_joke_text.png\">\n",
    "It is difficult to determine, visually, an ideal, objective number of clusters for categorizing jokes. After running our k-means algorithm for 10 clusters, the produced word clouds were incomprehensible and there wasn’t a clear distinction in types of jokes. By increasing the number of clusters, there will of course be clusters that do not make sense but will allow more room to create clusters that do. For example, there are jokes about “difference between engineers” or “screwing in lightbulbs.”\n",
    "\n",
    "The plotted elbow method is relatively smooth and seems parabolic. We determined that after 20 clusters, the lowered score was negligible. In order to visualize the cluster, we plotted the first two principal components with the data points colored according to their respective cluster assignment.\n",
    "<img src = \"./images/Kmeans_Clustering_on_Text_Data.png\">\n",
    "We extracted the top words in each cluster and created a wordcloud of each one. A wordcloud is a visualization in which the highest frequency words of each cluster is plotted in order of descending frequency and text size. Some clusters have nonsensical clustering of words\n",
    "<img src=\"images/Cluster19.png\">\n",
    "However, others have a clear 'theme' or 'pattern'. For example:\n",
    "<img src=\"images/Cluster3.png\">\n",
    "Is clearly about engineer jokes.\n",
    "<img src=\"images/Cluster4.png\">\n",
    "This one is clearly 'screw in lightbulb' jokes. After creating 20 clusters, we subjectively decided which topics warranted a tangible cluster. Some of these joke clusters are:\n",
    "\n",
    " - Religion\n",
    " - Marriage\n",
    " - Man walks into bar \n",
    " - Screwing in a lightbulb\n",
    " - Engineering \n",
    " - Doctor has bad news\n",
    "\n",
    "Using TF-IDF word vectorization, NMF matrix decomposition, and k-means clustering, we were able to extract a number of tangible, albeit subjective, topics of jokes from the unstructured text data. We can further use this to characterize user behavior by analyzing what type of joke genre a person prefers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA + User Behavior Clustering**\n",
    "\n",
    "[Source Code Notebook](user-rating-matrix-decomp.ipynb)\n",
    "\n",
    "Our data is a high dimensional, sparse matrix with values ranging on a continuous scale between -10.0 and 10.0. In order to visualize our data, we performed principal component analysis (PCA), a statistical procedure that finds orthonormal transformations of variables called principal components. We performed an associated matrix factorization technique known as truncated singular value decomposition (tSVD) to decrease computation time and complexity. By using SVD, we cut our computation time in half. SVD decomposes our user-ratings X matrix into 3 matrices according to:\n",
    "\n",
    "$$ X = (U\\Sigma)\\, V^T = W\\, V^T $$\n",
    "\n",
    "Where U and V are orthonormal and $\\Sigma$ is a diagonal matrix.\n",
    "We looked at the highest and lowest loadings for PC1, and determined that PC1 was used to group users who liked long jokes and short one-liners. Refer to the source notebook to see the jokes with the highest and lowest values in PC1.\n",
    "Upon computing tSVD on our user-ratings matrix, we plotted the cumulative variance explained as a function of total components.\n",
    "<img src=\"images/PCA_users.png\">\n",
    "Upon inspection, 28 principal components were needed to describe 85% of all variability of data in the original matrix. A strong elbow curve indicated that each new component added positively increased the total variance explained, and is expected when using tSVD. We chose to proceed with only the first 28 principal components for use in clustering. By plotting the first two components, we can see 3-4 different clusters by visual inspection.\n",
    "<img src=\"images/tSVD_on_User_rat_mat.png\">\n",
    "tSVD is an appropriate method to use for clustering methods even though the matrices may have negative values. These negative values do not take away from the interpretability of our results.\n",
    "\n",
    "Our next challenge was discerning different types of users present in our body of data. From our decomposed matrix, a k-means algorithm was used to find appropriate clusters. We experimented with different number of clusters, and plotted the score of each cluster fit to find the optimal k-value. The elbow method was again used for us to see the k-value with the highest in between cluster separation and lowest inter-cluster variation. \n",
    "<img src=\"./images/elbow_method_kmeans_users.png\">\n",
    "Past 6 clusters, the reduction is score is not as substantial, so we arrived at the choice of k = 6. By plotting the first two principal components and coloring in the data points according to the clusters assigned, we can see distinct groups of users:\n",
    "<img src = \"./images/Kmeans_Clustering_on_User_Data.png\">\n",
    "Each color represents a grouping of users into clusters based on their joke ratings.\n",
    "\n",
    "We then extracted the users from cluster 0 and inspected the jokes that they rated the highest and lowest. The highest rated jokes from cluster 0 were very dry humor:\n",
    "\n",
    "*A third man arrives at the gates. \"Religion?\" \"Jewish.\"*\n",
    "\n",
    "*\"Go to room 11, but be very quiet as you pass room 8.\"*\n",
    "\n",
    "*The man says, \"I can understand there being different rooms for different religions, but why\n",
    "must I be quiet when I pass room 8?\" St. Peter tells him, \"Well the Catholics are in room 8, \n",
    "and they think they're the only ones here.*\n",
    "\n",
    "Meanwhile the lowest rated jokes were all one liners like this:\n",
    "\n",
    "*How many teddybears does it take to change a lightbulb?*\n",
    "\n",
    "*It takes only one teddybear, but it takes a whole lot of lightbulbs.*\n",
    "\n",
    "We have sucessfully grouped users together based on their joke preference using K-means clustering. Later, we will be applying a matrix factorization method to a specific cluster of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbor Recommendations**\n",
    "\n",
    "[Source Code](recommender-knn.py)\n",
    "\n",
    "This program is completely runable and will recommend jokes based on your inputted ratings!\n",
    "\n",
    "The simplest algorithm computes cosine or correlation similarity of rows (users) or columns (items) and recommends items that k-nearest neighbors enjoyed as well. This is a naive approach with its own flaws, but is the easiest to implement. We compute the distance between each neighbor using cosine similarity. NaN values were dealt with by using a scaled value matrix, with all ratings between 1 and 2, and setting NaNs to zero. We justify our use of 0 to replace NaN values because cosine similarity is defined as:\n",
    "\n",
    "$$sim(a,b) = \\frac{a \\cdot b}{||a||*||b||}$$\n",
    "\n",
    "For example, if given two users a and b, who have rated five jokes:\n",
    "\n",
    "$$\n",
    "a = [0,0,7,9.2,2]\n",
    "$$$$\n",
    "b = [1,3,4.3,0,0]\n",
    "$$\n",
    "\n",
    "The dot product between them would be:\n",
    "\n",
    "$$\n",
    "a\\cdot b = 0*1 + 0*3 + 7*4.3 + 9.2*0 + 0*2\n",
    "$$\n",
    "\n",
    "The interpretation is that, no matter how poorly or how well user a rates a joke, if user b has not yet rated that joke, then the contribution of that joke to the distance between the two is 0.\n",
    "\n",
    "We initially found the user’s nearest 10% of neighbors to avoid finding similar neighbors strictly on the basis of having rated a low number of jokes, but when we examined the average number of jokes rated by the 10% of neighbors and by the 1% of neighbors, we found that they were virtually the same(both were right around 30 jokes). Since we were sure that even the 1% of nearest neighbors had rated at least around 30 jokes, which was similar to the average jokes rated by 10% of neighbors, we chose to find the new user’s 1% of neighbors, which would be more precise and less computationally expensive. After finding the new user’s nearest 1% of neighbors we compute the neighbors’ average ratings for all the jokes and recommend the joke with the highest average rating from the user’s neighbors. To ensure that we are recommending a new joke every time to the user, we keep track of the jokes the user has seen and recommend them the top joke that they haven’t seen yet. For a new user, our algorithm initiates a cold start by recommending two jokes at random from the set of 100.\n",
    "\n",
    "As a user, they are probably looking for jokes that they haven’t seen or jokes with different topics. Recommending jokes with diverse topics allow users to explore different tastes and keeps user engaged with the recommender product. On the other hand, lack of diversity will make users bored and less engaged with the product. There are two large problems with the relatively naive KNN recommender system:\n",
    "\n",
    "1. The recommender is prone to recommend popular items\n",
    "\n",
    "2. Recommender fails to recommend less-known items because those items have very little interactions\n",
    "\n",
    "3. As we acquire more and more data, the computation time and complexity will increase\n",
    "\n",
    "In spite of these problems, a KNN based recommender system is a good jumping off point and is easy to fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix Factorization Joke Recommendation**\n",
    "\n",
    "[Source Code Notebook](matrix-factorization-recommender.ipynb)\n",
    "\n",
    "In order to properly implement our matrix factorization method, we had to use our scaled value matrix, which set all the NaN values to zero. This way, we should be able to avoid any possible computational errors in the future. We also worked with a subset of 2,000 users instead of the original 73,421 users. Not only will this be more time and memory efficient, but it should also have enough information to display accurate predictions. In order to optimize our data, we wanted to find matrices U and V that correlate to our data after performing a dot product on them. By doing so, we performed dimensionality reduction with 5 latent factors. \n",
    "\n",
    "- In recommender system, we want to find $U$ and $V$ that minimize the residual:\n",
    "\\begin{align*}\n",
    "\\min_{U,V} \\|R - UV^T\\|_F^2 \n",
    "\\end{align*}\n",
    "\n",
    "- However, due to missing values, we minimize over just the observed ratings: i.e.,\n",
    "\\begin{align*}\n",
    "\\min_{U,V} f(U,V) &= \\min_{U,V} \\left\\{ \\sum_{r_{im}\\in R} f_{im}(u_i, v_m) \\right\\}\\\\\n",
    "&= \\min_{U,V} \\left\\{ \\sum_{r_{im}\\in R} (r_{im} - u_i^T v_m)^2 \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "Our intentions are to minimize the residuals calculated by the dot product of U and V, but since our data includes missing ratings (denoted as zeros), we have to minimize only over our observed ratings to remove the empty observed ratings in our predictions data. After performing the Frobenius norm onto our errors that were calculated before optimization, we were able to obtain a list of our repeating residual number of 1638.96. After calculating our residuals and optimizing our U and V matrices 300 times, we found that the residual values started to plateau at the 240 mark after being run a certain amount of times.\n",
    "<img src = \"./images/residual_opt.png\">\n",
    "After optimizing our residuals, we then ran our matrix factorization optimization 3 times instead of 300, immensely reducing our computation time from 30 minutes to 20 seconds. Thus, we are able to fit our predicted model to our observed ratings. \n",
    "<img src = \"./images/MF_Movie_heatmap.png\">\n",
    "Since we cannot create a heatmap with our subset of 2000 users and 100 jokes, we instead visualized our predicted values on 15 users (Users 14515-14529) and 11 jokes (Jokes 69-79). We specifically chose this data to work with because there were enough observed and empty ratings to better illustrate our optimization process. \n",
    "The Ratings heatmap represents our observed ratings, with the darkest color being the highest rating, 10, and our lightest color being an empty rating. The Fit-Ratings plot represents our predicted values fitted onto our observed ratings, which accounts for the jokes that have not yet been rated. The All-Ratings plot represents all of our predicted values, before removing the empty ratings. Finally, the Fit-Residuals plot shows the error term between our predicted values and observed ratings. Since the majority of the heatmap is green, it means that the error terms are relatively small. In addition, if we want to obtain recommendations for a new user, we would simply add a new user row with their inputted ratings and re-decompose the matrices. \n",
    "\n",
    "Now, we will compute the predictions for users that classified under cluster 0 using the same optimization method. Our goal is that we successfully predict the jokes with the highest ratings to be those from cluster 0.\n",
    "<img src = \"./images/MF_Movie_heatmap_cluster0.png\">\n",
    "After finding and fitting the predictions, we found that the highest rated jokes from these users fall under clusters 0 and 11. \n",
    "\n",
    "Thus, we have created a working recommender system based off of matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Considerations\n",
    "\n",
    "Computation was done on UCSB LSIT's GCP instance. Full list of libraries used are in the source code notebooks provided.\n",
    "\n",
    "**NaN values** \n",
    "\n",
    "We used PCA dimension reduction for visualization of our k-means clustering. We got rid of NaN values by using scikit learn MinMaxScaler, scaling from values 1 to 2, setting all NaN values to 0. This scaling and removal of NaN values does not take away from the interpretability of clusters nor the visualization of the first two principal components. We just wanted a visualization of our high dimensional data. \n",
    "When we did the matrix recommender method, we had to take a different approach. NaN values were avoided by creating a map of NaN and non-NaN values. Every time we computed the matrix multiplication, we applied this mask to our operations and updated the matrix accordingly.\n",
    "\n",
    "**PCA/NMF**\n",
    "\n",
    "PCA and NMF were carried out by sk-learn’s Truncated_SVD and NMF functions.\n",
    "\n",
    "**KMeans Clustering**\n",
    "\n",
    "We used KMeans Clustering to group the jokes themselves into clusters based on how frequently the words appear in the jokes. The jokes within each cluster share characteristics such as topic or tone.\n",
    "We also used KMeans Clustering to separate the users into groups based on their patterns of rating jokes. \n",
    "\n",
    "**KNN**\n",
    "\n",
    "For our KNN Joke recommender system, we asked a user to rate two random jokes, then found the user’s nearest neighbors based on those two ratings. We then recommended the user a new joke based on its neighbors.\n",
    "Matrix Recommendation\n",
    "We used the numpy library to carry out the matrix factorization, and used seaborn to create visualisations of the ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "With the recommendation systems that we have created, we are limited to the 100 jokes of the dataset we observed. However, there is potential for a more accurate and proficient system. For example, more jokes can be added to the data or new jokes can be written. With these jokes, they can be placed into either new or already existing joke clusters. We can then see how highly users would rate them based on which cluster of jokes they fall into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We were able to use NMF and K-means to find about ten discernible topics of jokes based on the text of the jokes. Examples of joke topics included: light bulb jokes, engineering jokes, marriage jokes, etc. We used PCA and K-means on the user-rating matrix and discovered 6 different types of users, such as users who liked dark jokes, puns, childish jokes, etc. We also built a recommender system for a new user using KNN which found similar users to the new user and recommended the similar users’ favorite jokes to the new user. We also used matrix factorization to predict joke ratings for all of the jokes for all of the users. Finally, using the clusters of users generated from K-means we used matrix factorization to predict how a specific cluster would rate all of the jokes. Using the clusters of jokes, we used matrix factorization to predict how users would rate jokes from only one cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1. G. Adomavicius, A. Tuzhilin, \"Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions\", IEEE Trans. Knowl. Data Eng., vol. 17, no. 6, pp. 734-749, Jun. 2005.\n",
    "2. An Efficient Non-Negative Matrix-Factorization-Based Approach to Collaborative Filtering for Recommender Systems - IEEE Journals & Magazine https://ieeexplore.ieee.org/abstract/document/6748996 (accessed Dec 10, 2019)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
