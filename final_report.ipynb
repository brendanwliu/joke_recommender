{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joke Recommender\n",
    "Brendan Liu, Elaine Sieng, Josh Kim, Yerem Istanboulian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from IPython.display import Image\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import text\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Recommender systems are techniques that attempt to suggest relevant items to users from very large and complex datasets. The dataset used includes over 4.1 million continuous ratings (-10.00 to +10.00) of 100 jokes from 73,421 users collected by Ken Goldberg and Theresa Roeder and Dhruv Gupta and Chris Perkins at UC Berkeley. Specifically, our joke recommender system will implement collaborative filtering methods to filter items that a user may like based on similar users. Using model based approaches uncover user-joke interactions which are then used to make further recommendations. This report will discuss specific matrix decomposition methods, component analysis and use a K-nearest neighbors approach to compare to a classic matrix approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Recommender systems are quintessential in the connected world. Many different businesses often face the arduous task of recommending items to users without knowing the preferences of said users. Starting in the 1990’s research in recommender systems have grown significantly and have developed great efficiency in predicting the preferences of users.1 According to Zhu et al., a recommender system is based on three important pieces of data: users, items (e.g., movies, jokes, news), and user-item usage history (e.g., ratings). The goal is to attempt to understand the relationship between the users, items and user-item history, and to use that understanding to predict future user-item interactions.2 To tackle these tasks, over the last three decades, researchers have developed methods like non-negative matrix factorization (NMF), K-nearest neighbors (KNN), neural autoencoders, and many more. These models can be further grouped into several categories, among which a very important branch is collaborative filtering (CF).\n",
    "\n",
    "CF can be implemented using many different approaches. The central data source of a CF method is the user-review matrix, with each entry corresponding to a numerical rating of an item within a finite itemset. These matrices tend to be extremely sparse, with many users and items. Our application of matrix methods attempts to find and extract a low ranking representation of the original user-ratings matrix.\n",
    " \n",
    "In our work, we explore three different matrix decomposition methods for topic discovery and user clustering; and, we also utilize a KNN approach to recommending jokes to users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Reading in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../joke_recommender/data/'\n",
    "df = pd.read_csv(input_path + 'joke_dataframe.csv')\n",
    "df = df.drop(['Unnamed: 0', 'JokeId'], axis = 1)\n",
    "joke_frame = pd.read_csv(input_path + 'JokeText.csv')\n",
    "jokes = joke_frame.JokeText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the head of the user-ratings matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User1</th>\n",
       "      <th>User2</th>\n",
       "      <th>User3</th>\n",
       "      <th>User4</th>\n",
       "      <th>User5</th>\n",
       "      <th>User6</th>\n",
       "      <th>User7</th>\n",
       "      <th>User8</th>\n",
       "      <th>User9</th>\n",
       "      <th>User10</th>\n",
       "      <th>...</th>\n",
       "      <th>User73412</th>\n",
       "      <th>User73413</th>\n",
       "      <th>User73414</th>\n",
       "      <th>User73415</th>\n",
       "      <th>User73416</th>\n",
       "      <th>User73417</th>\n",
       "      <th>User73418</th>\n",
       "      <th>User73419</th>\n",
       "      <th>User73420</th>\n",
       "      <th>User73421</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>-8.79</td>\n",
       "      <td>-3.50</td>\n",
       "      <td>7.14</td>\n",
       "      <td>-8.79</td>\n",
       "      <td>9.22</td>\n",
       "      <td>-4.03</td>\n",
       "      <td>3.11</td>\n",
       "      <td>-3.64</td>\n",
       "      <td>-7.67</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.90</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-2.91</td>\n",
       "      <td>-3.88</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>9.37</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>-5.15</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-3.06</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-3.93</td>\n",
       "      <td>-3.64</td>\n",
       "      <td>7.52</td>\n",
       "      <td>-6.46</td>\n",
       "      <td>-3.25</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>-4.61</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8.98</td>\n",
       "      <td>9.27</td>\n",
       "      <td>-6.99</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.39</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.26</td>\n",
       "      <td>7.67</td>\n",
       "      <td>3.45</td>\n",
       "      <td>5.44</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>1.26</td>\n",
       "      <td>4.03</td>\n",
       "      <td>...</td>\n",
       "      <td>3.64</td>\n",
       "      <td>4.32</td>\n",
       "      <td>6.99</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-8.40</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>9.51</td>\n",
       "      <td>-7.67</td>\n",
       "      <td>-1.60</td>\n",
       "      <td>8.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>1.26</td>\n",
       "      <td>6.65</td>\n",
       "      <td>8.25</td>\n",
       "      <td>-8.11</td>\n",
       "      <td>-6.75</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.84</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.76</td>\n",
       "      <td>1.60</td>\n",
       "      <td>-5.39</td>\n",
       "      <td>-7.52</td>\n",
       "      <td>4.08</td>\n",
       "      <td>4.42</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-3.01</td>\n",
       "      <td>5.19</td>\n",
       "      <td>...</td>\n",
       "      <td>4.56</td>\n",
       "      <td>6.21</td>\n",
       "      <td>6.99</td>\n",
       "      <td>5.15</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-3.64</td>\n",
       "      <td>-4.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.29</td>\n",
       "      <td>-1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.50</td>\n",
       "      <td>7.28</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-5.87</td>\n",
       "      <td>8.06</td>\n",
       "      <td>-6.65</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.87</td>\n",
       "      <td>-6.36</td>\n",
       "      <td>6.70</td>\n",
       "      <td>-9.37</td>\n",
       "      <td>-3.74</td>\n",
       "      <td>-7.23</td>\n",
       "      <td>8.59</td>\n",
       "      <td>-8.79</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-2.57</td>\n",
       "      <td>-8.69</td>\n",
       "      <td>-8.40</td>\n",
       "      <td>-5.15</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>9.08</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>-1.41</td>\n",
       "      <td>-4.66</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-7.14</td>\n",
       "      <td>2.48</td>\n",
       "      <td>9.13</td>\n",
       "      <td>-5.19</td>\n",
       "      <td>7.52</td>\n",
       "      <td>1.36</td>\n",
       "      <td>8.83</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 73422 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   User1  User2  User3  User4  User5  User6  User7  User8  User9  User10  ...  \\\n",
       "0   5.10  -8.79  -3.50   7.14  -8.79   9.22  -4.03   3.11  -3.64   -7.67  ...   \n",
       "1   4.90  -0.87  -2.91  -3.88  -0.58   9.37  -1.55   0.92  -3.35   -5.15  ...   \n",
       "2   1.75   1.99  -2.18  -3.06  -0.58  -3.93  -3.64   7.52  -6.46   -3.25  ...   \n",
       "3  -4.17  -4.61  -0.10   0.05   8.98   9.27  -6.99   0.49  -3.40   -1.65  ...   \n",
       "4   5.15   5.39   7.52   6.26   7.67   3.45   5.44  -0.58   1.26    4.03  ...   \n",
       "5   1.75  -0.78   1.26   6.65   8.25  -8.11  -6.75   2.14   0.34    1.84  ...   \n",
       "6   4.76   1.60  -5.39  -7.52   4.08   4.42  -0.15  -0.24  -3.01    5.19  ...   \n",
       "7   3.30   1.07   1.50   7.28   2.52   2.72  -5.87   8.06  -6.65   -0.29  ...   \n",
       "8  -2.57  -8.69  -8.40  -5.15  -9.66   9.08  -3.54   2.82  -3.40   -3.54  ...   \n",
       "9  -1.41  -4.66   4.37  -7.14   2.48   9.13  -5.19   7.52   1.36    8.83  ...   \n",
       "\n",
       "   User73412  User73413  User73414  User73415  User73416  User73417  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4       3.64       4.32       6.99      -9.66      -8.40      -0.63   \n",
       "5        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "6       4.56       6.21       6.99       5.15       4.37      -3.64   \n",
       "7       0.87      -6.36       6.70      -9.37      -3.74      -7.23   \n",
       "8        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "9        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "   User73418  User73419  User73420  User73421  \n",
       "0        NaN        NaN        NaN        NaN  \n",
       "1        NaN        NaN        NaN        NaN  \n",
       "2        NaN        NaN        NaN        NaN  \n",
       "3        NaN        NaN        NaN        NaN  \n",
       "4       9.51      -7.67      -1.60       8.30  \n",
       "5        NaN        NaN        NaN        NaN  \n",
       "6      -4.95        NaN      -5.29      -1.36  \n",
       "7       8.59      -8.79      -3.69       0.29  \n",
       "8        NaN        NaN        NaN        NaN  \n",
       "9        NaN        NaN        NaN        NaN  \n",
       "\n",
       "[10 rows x 73422 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "**KMeans Clustering of Text - Topic Discovery**\n",
    "\n",
    "To make the joke text machine readable, we used the TF-IDF vectorization technique. After vectorizing and normalizing the term-matrix, we used NMF to decompose this matrix instead of truncated singlular value decomposition (tSVD). This is because there is no interpretability of the negative values in tSVD. NMF ensures that the resulting decomposed term frequency matrix has interpretable positive values. NMF decomposes the matrix according to this algorithm:\n",
    "\n",
    "$$ \\min_{W,H} \\| X - WH \\|_F\\\\\n",
    "\\text{ subject to } W\\geq 0,\\ H\\geq 0, $$  \n",
    "    \n",
    "where $W$ is ${p\\times r}$ matrix and $H$ is ${r\\times n}$ matrix. This also ensures only positive values in the resulting matrices.\n",
    "    \n",
    "After implementing NMF, we again passed the decomposed W column into the K-means clustering algorithm. We plot the score versus the number of clusters to choose the optimal number of clusters required.\n",
    "<img src = \"./images/elbow_method_joke_text.png\">\n",
    "The plotted elbow method is relatively smooth and seems parabolic. We determined that after 20 clusters, the lowered score was negligible. We plotted the first two principal components with the data points colored according to their respective cluster assignment.\n",
    "<img src = \"./images/Kmeans_Clustering_on_Text_Data.png\">\n",
    "We extracted the top words in each cluster and created a wordcloud of each one. A wordcloud is a visualization in which the highest frequency words of each cluster is plotted in order of descending frequency and text size. Some clusters have nonsensical clustering of words\n",
    "<img src=\"images/Cluster19.png\">\n",
    "However, others have a clear 'theme' or 'pattern'. For example:\n",
    "<img src=\"images/Cluster3.png\">\n",
    "Is clearly about engineer jokes.\n",
    "<img src=\"images/Cluster4.png\">\n",
    "This one is clearly 'screw in lightbulb' jokes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA + User Clustering**\n",
    "\n",
    "Our data is a high dimensional, sparse matrix with values ranging on a continuous scale between -10.0 and 10.0. In order to visualize our data, we performed principal component analysis (PCA), a statistical procedure that finds orthonormal transformations of variables called principal components. We performed an associated matrix factorization technique known as truncated singular value decomposition (tSVD) to decrease computation time and complexity. By using SVD, we cut our computation time in half. \n",
    "Upon computing tSVD on our user-ratings matrix, we plotted the cumulative variance explained as a function of total components.\n",
    "<img src=\"images/PCA_users.png\">\n",
    "By inspection, 28 principal components were needed to describe 85% of all variability in the original matrix. A strong elbow curve indicated that each new component added positively increase the total variance explained, and is expected using tSVD. We chose to proceed with only the first 28 principal components for use in clustering. By plotting the first two components, we can see 3-4 different clusters by visual inspection.\n",
    "<img src=\"images/tSVD_on_User_rat_mat.png\">\n",
    "tSVD is an appropriate method to use for clustering methods even though the matrices may have negative values. These negative values do not take away from the interpretability of our results.\n",
    "\n",
    "Our next challenge was discerning different topics of jokes present in our body of joke text. From our decomposed matrix, a k-means algorithm was used to find appropriate clusters. We experimented with different number of clusters, and plotted the score of each cluster fit to find the optimal k-value. The elbow method allows us to see the k-value with the highest in between cluster separation and lowest inter-cluster variation.\n",
    "<img src=\"./images/elbow_method_kmeans_users.png\">\n",
    "Past 6 clusters, the reduction is score is not as substantial, so we arrived at the choice of k = 6. By plotting the first two principal components and coloring in the data points according to the clusters assigned, we can see distinct groups of users.\n",
    "<img src = \"./images/Kmeans_Clustering_on_User_Data.png\">\n",
    "The grey points represents the center of each cluster, and all data points are colored according to their assigned cluster. Extracting the 5 highest rated jokes from each cluster, we get these joke indices:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "& Rank1 & Rank2 & Rank3 & Rank4 & Rank5 \\\\\n",
    "Cluster\\ 1& 0 & 7 & 6 & 3 & 8\\\\\n",
    "Cluster\\ 2 & 0 & 2 & 3 & 5 & 7\\\\\n",
    "Cluster\\ 3 & 0 & 1 & 2 & 6 & 62 \\\\\n",
    "Cluster\\ 4 & 0 & 1 & 3 & 4 & 13 \\\\\n",
    "Cluster\\ 5 & 0 & 4 & 90 & 18 & 64\\\\\n",
    "Cluster\\ 6 & 0 & 3 & 15 & 5 & 10\n",
    "\\end{matrix}$$\n",
    "This is very  interesting, because each cluster rated joke 0 highly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbor Recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix Factorization Joke Recommendation**\n",
    "<img src = \"./images/residual_opt.png\">\n",
    "<img src = \"./images/MF_Movie_heatmap.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
